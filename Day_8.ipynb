{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Topic >\n",
    "\n",
    "1. review\n",
    "\n",
    "\n",
    "2. topic\n",
    "\n",
    "    2.1 LDA\n",
    "    \n",
    "    2.2 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 review 이용 카운트 기반 단어 표현: BoW, DTM, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 review data 로드 및 내용 list로 변환\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "movie_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>story</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>그린 북</td>\n",
       "      <td>1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>가버나움</td>\n",
       "      <td>나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 1...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>베일리 어게인</td>\n",
       "      <td>귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다...</td>\n",
       "      <td>모험</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>주전장</td>\n",
       "      <td>일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인...</td>\n",
       "      <td>다큐멘터리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>포드 V 페라리</td>\n",
       "      <td>1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한...</td>\n",
       "      <td>액션</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     title                                              story  \\\n",
       "0           0      그린 북  1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아...   \n",
       "1           1      가버나움  나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 1...   \n",
       "2           2   베일리 어게인  귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다...   \n",
       "3           3       주전장  일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인...   \n",
       "4           4  포드 V 페라리  1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한...   \n",
       "\n",
       "   genre  \n",
       "0    드라마  \n",
       "1    드라마  \n",
       "2     모험  \n",
       "3  다큐멘터리  \n",
       "4     액션  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>story</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그린 북</td>\n",
       "      <td>1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>가버나움</td>\n",
       "      <td>나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 1...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>베일리 어게인</td>\n",
       "      <td>귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다...</td>\n",
       "      <td>모험</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>주전장</td>\n",
       "      <td>일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인...</td>\n",
       "      <td>다큐멘터리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>포드 V 페라리</td>\n",
       "      <td>1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한...</td>\n",
       "      <td>액션</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>바그다드 카페 : 디렉터스컷</td>\n",
       "      <td>황량한 사막 한가운데 자리 잡은 초라한 바그다드 카페 커피머신은 고장난지 오래고 먼...</td>\n",
       "      <td>코미디</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>론 서바이버</td>\n",
       "      <td>2005년 6월 28일 아프가니스탄에서 복무중인 네이비씰 대원 마커스 마이클 대니 ...</td>\n",
       "      <td>액션</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>국제시장</td>\n",
       "      <td>1950년대 한국전쟁 이후로부터 현재에 이르기까지 격변의 시대를 관통하며 살아온 우...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>말레나</td>\n",
       "      <td>2차 대전이 한창인 햇빛 찬란한 지중해의 작은 마을 매혹적인 말레나 걸어갈 때면 어...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>콜드 마운틴</td>\n",
       "      <td>남군 병사인 인만주드 로은 전투 중에 중상을 입고 병원에 입원해있던 도중에 탈영을 ...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               title                                              story  genre\n",
       "0               그린 북  1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아...    드라마\n",
       "1               가버나움  나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 1...    드라마\n",
       "2            베일리 어게인  귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다...     모험\n",
       "3                주전장  일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인...  다큐멘터리\n",
       "4           포드 V 페라리  1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한...     액션\n",
       "..               ...                                                ...    ...\n",
       "379  바그다드 카페 : 디렉터스컷  황량한 사막 한가운데 자리 잡은 초라한 바그다드 카페 커피머신은 고장난지 오래고 먼...    코미디\n",
       "380           론 서바이버  2005년 6월 28일 아프가니스탄에서 복무중인 네이비씰 대원 마커스 마이클 대니 ...     액션\n",
       "381             국제시장  1950년대 한국전쟁 이후로부터 현재에 이르기까지 격변의 시대를 관통하며 살아온 우...    드라마\n",
       "382              말레나  2차 대전이 한창인 햇빛 찬란한 지중해의 작은 마을 매혹적인 말레나 걸어갈 때면 어...    드라마\n",
       "383           콜드 마운틴  남군 병사인 인만주드 로은 전투 중에 중상을 입고 병원에 입원해있던 도중에 탈영을 ...    드라마\n",
       "\n",
       "[384 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불필요한 인덱스 삭제\n",
    "\n",
    "movie_df.drop(movie_df.columns[[0]], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄거리만 담아 list화\n",
    "\n",
    "movie_story = movie_df.story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = movie_story.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아함 그 자체인천재 피아니스트 돈 셜리마허샬라 알리 박사의 운전기사 면접을 보게 된다백악관에도 초청되는 등 미국 전역에서 콘서트 요청을 받으며 명성을 떨치고 있는 돈 셜리는위험하기로 소문난 미국 남부 투어 공연을 떠나기로 결심하고투어 기간 동안 자신의 보디가드 겸 운전기사로 토니를 고용한다거친 인생을 살아온 토니 발레롱가와 교양과 기품을 지키며 살아온 돈 셜리 박사생각 행동 말투 취향까지 달라도 너무 다른 두 사람은그들을 위한 여행안내서 그린북에 의존해 특별한 남부 투어를 시작하는데',\n",
       " '나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 12살 소년 자인으로부터',\n",
       " '귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다시 시작된 견생 2회차 아니 3회차1등 경찰견 엘리에서 찰떡같이 마음을 알아주는 소울메이트 티노까지다시 태어날 때마다 성별과 생김새 직업에 이름도 바뀌지만여전히 영혼만은 사랑 충만 애교 충만 주인바라기 베일리어느덧 견생 4회차 방랑견이 되어 떠돌던 베일리는마침내 자신이 돌아온 진짜 이유를 깨닫고 어딘가로 달려가기 시작하는데',\n",
       " '일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인 유튜버 미키 데자키 그는 일본군위안부에 관한 기사를 쓴 기자가 우익들에게 인신공격 당하는 것을 보며 왜 그토록 이들이 이 문제를 감추려고 하는지 궁금해졌다호기심을 안고 찾아간 그들은 무시무시한 이야기를 전하고 그들의 주장을 반격하는 또 다른 인물들을 만나면서 숨겨진 비밀을 발견하게 되는데 숨 쉴 틈 없는 전쟁이 시작된다',\n",
       " '1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한 절대적 1위 페라리와의 인수 합병을 추진한다막대한 자금력에도 불구 계약에 실패하고 엔초 페라리로부터 모욕까지 당한 헨리 포드 2세는르망 24시간 레이스에서 페라리를 박살 낼 차를 만들 것을 지시한다 불가능을 즐기는 두 남자를 주목하라세계 3대 자동차 레이싱 대회이자 지옥의 레이스로 불리는 르망 24시간 레이스출전 경험조차 없는 포드는 대회 6연패를 차지한 페라리에 대항하기 위해르망 레이스 우승자 출신 자동차 디자이너 캐롤 셸비맷 데이먼를 고용하고 그는 누구와도 타협하지 않지만 열정과 실력만큼은 최고인 레이서 켄 마일스크리스찬 베일를 자신의 파트너로 영입한다포드의 경영진은 제 멋대로인 켄 마일스를 눈엣가시처럼 여기며자신들의 입맛에 맞춘 레이스를 펼치기를 강요하지만두 사람은 어떤 간섭에도 굴하지 않고 불가능을 뛰어넘기 위한 질주를 시작하는데그 어떤 각본보다 놀라운 실화가 펼쳐진다']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반 단어 표현 (Count-based word Representation): BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나를',\n",
       " '세상',\n",
       " '에',\n",
       " '태어나게',\n",
       " '한',\n",
       " '부모님',\n",
       " '을',\n",
       " '고소하고',\n",
       " '싶어요',\n",
       " '출생',\n",
       " '기록',\n",
       " '조차',\n",
       " '없이',\n",
       " '살아온',\n",
       " '어쩌면',\n",
       " '12',\n",
       " '살',\n",
       " '소년',\n",
       " '자인',\n",
       " '으로부터']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 카운트 기반 단어 표현 (Count-based word Representation) - BoW\n",
    "# 1. 파이썬 기초 코드\n",
    "# 토큰화\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "movie_review_tokens = []\n",
    "\n",
    "for review in movie_reviews:\n",
    "    movie_review_tokens.append(okt.morphs(review))\n",
    "    \n",
    "movie_review_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['세상', '부모님', '출생', '기록', '어쩌면', '살', '소년', '자인']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_review_nouns = []\n",
    "\n",
    "for review in movie_reviews:\n",
    "    movie_review_nouns.append(okt.nouns(review))\n",
    "    \n",
    "movie_review_nouns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 각 단어에 대한 고유한 정수 인덱스 부여 - 명사,조사 (토큰)\n",
    "\n",
    "word2indexs = [] \n",
    "bows = []       \n",
    "\n",
    "for review_token in movie_review_tokens:\n",
    "    word2index = {}         # 단어별 인덱스 저장\n",
    "    bow = []                # 등장 횟수 vector\n",
    "    for voca in review_token:\n",
    "        if voca not in word2index.keys():\n",
    "            word2index[voca] = len(word2index)  # word2index에 없는 단어는 새로 추가\n",
    "            bow.insert(len(word2index)-1, 1)   # bow 전체에 기본값 저장, 단어 등장횟수는 최소 1 이상\n",
    "        else:\n",
    "            index = word2index.get(voca)  # 재등장 단어의 인덱스 획득\n",
    "            bow[index] += 1    # 재등장 단어의 등장횟수 \n",
    "        \n",
    "    word2indexs.append(word2index)\n",
    "    bows.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1962년': 0,\n",
       " '미국': 1,\n",
       " '입담': 2,\n",
       " '과': 3,\n",
       " '주먹': 4,\n",
       " '만': 5,\n",
       " '믿고': 6,\n",
       " '살아가던': 7,\n",
       " '토니': 8,\n",
       " '발레': 9,\n",
       " '롱': 10,\n",
       " '가': 11,\n",
       " '비고': 12,\n",
       " '모텐슨는': 13,\n",
       " '교양': 14,\n",
       " '우아함': 15,\n",
       " '그': 16,\n",
       " '자체': 17,\n",
       " '인': 18,\n",
       " '천재': 19,\n",
       " '피아니스트': 20,\n",
       " '돈': 21,\n",
       " '셜리마허샬': 22,\n",
       " '라': 23,\n",
       " '알리': 24,\n",
       " '박사': 25,\n",
       " '의': 26,\n",
       " '운전기사': 27,\n",
       " '면접': 28,\n",
       " '을': 29,\n",
       " '보게': 30,\n",
       " '된다': 31,\n",
       " '백악관': 32,\n",
       " '에도': 33,\n",
       " '초청': 34,\n",
       " '되는': 35,\n",
       " '등': 36,\n",
       " '전역': 37,\n",
       " '에서': 38,\n",
       " '콘서트': 39,\n",
       " '요청': 40,\n",
       " '받으며': 41,\n",
       " '명성': 42,\n",
       " '떨치고': 43,\n",
       " '있는': 44,\n",
       " '셜리': 45,\n",
       " '는': 46,\n",
       " '위험하기로': 47,\n",
       " '소문난': 48,\n",
       " '남부': 49,\n",
       " '투어': 50,\n",
       " '공연': 51,\n",
       " '떠나기로': 52,\n",
       " '결심': 53,\n",
       " '하고': 54,\n",
       " '기간': 55,\n",
       " '동안': 56,\n",
       " '자신': 57,\n",
       " '보디가드': 58,\n",
       " '겸': 59,\n",
       " '로': 60,\n",
       " '를': 61,\n",
       " '고용': 62,\n",
       " '한': 63,\n",
       " '다': 64,\n",
       " '거친': 65,\n",
       " '인생': 66,\n",
       " '살아온': 67,\n",
       " '와': 68,\n",
       " '기품': 69,\n",
       " '지키며': 70,\n",
       " '생각': 71,\n",
       " '행동': 72,\n",
       " '말투': 73,\n",
       " '취향': 74,\n",
       " '까지': 75,\n",
       " '달라': 76,\n",
       " '도': 77,\n",
       " '너무': 78,\n",
       " '다른': 79,\n",
       " '두': 80,\n",
       " '사람': 81,\n",
       " '은': 82,\n",
       " '들': 83,\n",
       " '위': 84,\n",
       " '여행안내서': 85,\n",
       " '그린북': 86,\n",
       " '에': 87,\n",
       " '의존': 88,\n",
       " '해': 89,\n",
       " '특별한': 90,\n",
       " '시작': 91,\n",
       " '하는데': 92}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2indexs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 각 단어에 대한 고유한 정수 인덱스 부여 - 명사\n",
    "\n",
    "word2indexs_n = [] \n",
    "bows_n = []       \n",
    "\n",
    "for review_token in movie_review_nouns:\n",
    "    word2index = {}         # 단어별 인덱스 저장\n",
    "    bow = []                # 등장 횟수 vector\n",
    "    for voca in review_token:\n",
    "        if voca not in word2index.keys():\n",
    "            word2index[voca] = len(word2index)  # word2index에 없는 단어는 새로 추가\n",
    "            bow.insert(len(word2index)-1, 1)   # bow 전체에 기본값 저장, 단어 등장횟수는 최소 1 이상\n",
    "        else:\n",
    "            index = word2index.get(voca)  # 재등장 단어의 인덱스 획득\n",
    "            bow[index] += 1    # 재등장 단어의 등장횟수 \n",
    "        \n",
    "    word2indexs_n.append(word2index)\n",
    "    bows_n.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'미국': 0,\n",
       " '입담': 1,\n",
       " '주먹': 2,\n",
       " '토니': 3,\n",
       " '발레': 4,\n",
       " '롱': 5,\n",
       " '비고': 6,\n",
       " '모텐슨는': 7,\n",
       " '교양': 8,\n",
       " '그': 9,\n",
       " '자체': 10,\n",
       " '천재': 11,\n",
       " '피아니스트': 12,\n",
       " '돈': 13,\n",
       " '셜리마허샬': 14,\n",
       " '알리': 15,\n",
       " '박사': 16,\n",
       " '운전기사': 17,\n",
       " '면접': 18,\n",
       " '백악관': 19,\n",
       " '초청': 20,\n",
       " '등': 21,\n",
       " '전역': 22,\n",
       " '콘서트': 23,\n",
       " '요청': 24,\n",
       " '명성': 25,\n",
       " '셜리': 26,\n",
       " '남부': 27,\n",
       " '투어': 28,\n",
       " '공연': 29,\n",
       " '결심': 30,\n",
       " '기간': 31,\n",
       " '동안': 32,\n",
       " '자신': 33,\n",
       " '보디가드': 34,\n",
       " '겸': 35,\n",
       " '고용': 36,\n",
       " '인생': 37,\n",
       " '기품': 38,\n",
       " '생각': 39,\n",
       " '행동': 40,\n",
       " '말투': 41,\n",
       " '취향': 42,\n",
       " '달라': 43,\n",
       " '다른': 44,\n",
       " '두': 45,\n",
       " '사람': 46,\n",
       " '위': 47,\n",
       " '여행안내서': 48,\n",
       " '그린북': 49,\n",
       " '의존': 50,\n",
       " '시작': 51}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2indexs_n[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bows_n[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 카운트 기반 단어 표현 (Count-based word Representation) - BoW\n",
    "# 2. Scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector = CountVectorizer()  # BoW 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_word2indexs = []\n",
    "s_bows = []\n",
    "\n",
    "for review_token in movie_review_tokens:\n",
    "    s_bows.append(vector.fit_transform(review_token))  # 빈도 계산\n",
    "    s_word2indexs.append(vector.vocabulary_)     # 인덱스 값 부여                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'나를': 3,\n",
       " '세상': 6,\n",
       " '태어나게': 15,\n",
       " '부모님': 4,\n",
       " '고소하고': 1,\n",
       " '싶어요': 8,\n",
       " '출생': 14,\n",
       " '기록': 2,\n",
       " '조차': 13,\n",
       " '없이': 10,\n",
       " '살아온': 5,\n",
       " '어쩌면': 9,\n",
       " '12': 0,\n",
       " '소년': 7,\n",
       " '자인': 12,\n",
       " '으로부터': 11}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_word2indexs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (3, 15)\t1\n",
      "  (5, 4)\t1\n",
      "  (7, 1)\t1\n",
      "  (8, 8)\t1\n",
      "  (9, 14)\t1\n",
      "  (10, 2)\t1\n",
      "  (11, 13)\t1\n",
      "  (12, 10)\t1\n",
      "  (13, 5)\t1\n",
      "  (14, 9)\t1\n",
      "  (15, 0)\t1\n",
      "  (17, 7)\t1\n",
      "  (18, 12)\t1\n",
      "  (19, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print(s_bows[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 카운트 기반 단어 표현 (Count-based word Representation) - BoW\n",
    "# 3. 불용어 처리\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "with open('stopwords.txt', 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.append(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review_nouns_stop = []\n",
    "\n",
    "for review in movie_review_nouns:\n",
    "    results = []\n",
    "    for word in review:\n",
    "        if word not in stop_words:\n",
    "            results.append(word)\n",
    "            \n",
    "        movie_review_nouns_stop.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = []\n",
    "bows = []\n",
    "\n",
    "for review_token in movie_review_nouns_stop:\n",
    "    bows.append(vector.fit_transform(review_token))\n",
    "    word2index.append(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 단어 행렬 (Document-Term Matrix): DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문서 단어 행렬 (Document-Term Matrix) - DTM\n",
    "\n",
    "from math import log\n",
    "\n",
    "vocab = list(set(w for review in movie_reviews for w in review.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF를 위한 함수\n",
    "\n",
    "N = len(movie_reviews)  # 비교하려는 대상, DTM으로 구성하려는 대상\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for review in movie_reviews:\n",
    "        df += t in review\n",
    "        \n",
    "    return log(N / (df+1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>1000점을</th>\n",
       "      <th>100m도</th>\n",
       "      <th>100년전</th>\n",
       "      <th>100를</th>\n",
       "      <th>100만</th>\n",
       "      <th>100만이</th>\n",
       "      <th>100미터</th>\n",
       "      <th>100번째</th>\n",
       "      <th>...</th>\n",
       "      <th>힘없는</th>\n",
       "      <th>힘없이</th>\n",
       "      <th>힘에</th>\n",
       "      <th>힘으로</th>\n",
       "      <th>힘은</th>\n",
       "      <th>힘을</th>\n",
       "      <th>힘의</th>\n",
       "      <th>힘이</th>\n",
       "      <th>힘입어</th>\n",
       "      <th>힘차게</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 22441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  100  1000점을  100m도  100년전  100를  100만  100만이  100미터  100번째  ...  힘없는  \\\n",
       "0    1    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "1    1    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "2    1    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "3    0    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "4    2    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "..  ..  ...     ...    ...    ...   ...   ...    ...    ...    ...  ...  ...   \n",
       "379  0    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "380  0    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "381  1    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "382  1    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "383  0    0       0      0      0     0     0      0      0      0  ...    0   \n",
       "\n",
       "     힘없이  힘에  힘으로  힘은  힘을  힘의  힘이  힘입어  힘차게  \n",
       "0      0   0    0   0   0   0   0    0    0  \n",
       "1      0   0    0   0   0   0   0    0    0  \n",
       "2      0   0    0   0   0   0   0    0    0  \n",
       "3      0   0    0   0   0   0   0    0    0  \n",
       "4      0   0    0   0   0   0   0    0    0  \n",
       "..   ...  ..  ...  ..  ..  ..  ..  ...  ...  \n",
       "379    0   0    0   0   0   0   0    0    0  \n",
       "380    0   0    0   0   0   0   0    0    0  \n",
       "381    0   0    0   0   0   0   0    0    0  \n",
       "382    0   0    0   0   0   0   0    0    0  \n",
       "383    0   0    0   0   0   0   0    0    0  \n",
       "\n",
       "[384 rows x 22441 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DTM 생성\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = movie_reviews[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tf(t, d))\n",
    "        \n",
    "tf_ = pd.DataFrame(result, columns = vocab) ; tf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 빈도 - 역 문서 빈도 (Term Frequency - Inverse Document Frequency): TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>3.465736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000점을</th>\n",
       "      <td>5.257495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100m도</th>\n",
       "      <td>5.257495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100년전</th>\n",
       "      <td>5.257495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>힘을</th>\n",
       "      <td>2.906120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>힘의</th>\n",
       "      <td>4.158883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>힘이</th>\n",
       "      <td>4.158883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>힘입어</th>\n",
       "      <td>5.257495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>힘차게</th>\n",
       "      <td>5.257495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22441 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             IDF\n",
       "1       0.875469\n",
       "100     3.465736\n",
       "1000점을  5.257495\n",
       "100m도   5.257495\n",
       "100년전   5.257495\n",
       "...          ...\n",
       "힘을      2.906120\n",
       "힘의      4.158883\n",
       "힘이      4.158883\n",
       "힘입어     5.257495\n",
       "힘차게     5.257495\n",
       "\n",
       "[22441 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 단어 빈도 - 역 문서 빈도 (Term Frequency - Inverse Document Frequency): TF-IDF\n",
    "# IDF 값\n",
    "\n",
    "results = []\n",
    "\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    results.append(idf(t))\n",
    "    \n",
    "idf_ = pd.DataFrame(results, index = vocab, columns = ['IDF']) ; idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>1000점을</th>\n",
       "      <th>100m도</th>\n",
       "      <th>100년전</th>\n",
       "      <th>100를</th>\n",
       "      <th>100만</th>\n",
       "      <th>100만이</th>\n",
       "      <th>100미터</th>\n",
       "      <th>100번째</th>\n",
       "      <th>...</th>\n",
       "      <th>힘없는</th>\n",
       "      <th>힘없이</th>\n",
       "      <th>힘에</th>\n",
       "      <th>힘으로</th>\n",
       "      <th>힘은</th>\n",
       "      <th>힘을</th>\n",
       "      <th>힘의</th>\n",
       "      <th>힘이</th>\n",
       "      <th>힘입어</th>\n",
       "      <th>힘차게</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.750937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows × 22441 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1  100  1000점을  100m도  100년전  100를  100만  100만이  100미터  100번째  \\\n",
       "0    0.875469  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "1    0.875469  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "2    0.875469  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "3    0.000000  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "4    1.750937  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "..        ...  ...     ...    ...    ...   ...   ...    ...    ...    ...   \n",
       "379  0.000000  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "380  0.000000  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "381  0.875469  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "382  0.875469  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "383  0.000000  0.0     0.0    0.0    0.0   0.0   0.0    0.0    0.0    0.0   \n",
       "\n",
       "     ...  힘없는  힘없이   힘에  힘으로   힘은   힘을   힘의   힘이  힘입어  힘차게  \n",
       "0    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "379  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "380  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "381  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "382  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "383  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[384 rows x 22441 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(N):\n",
    "    results.append([])\n",
    "    review = movie_reviews[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        \n",
    "        results[-1].append(tfidf(t, review))\n",
    "\n",
    "tfidf_ = pd.DataFrame(results, columns = vocab) ; tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 유사도 (Document Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>story</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>그린 북</td>\n",
       "      <td>1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>가버나움</td>\n",
       "      <td>나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 1...</td>\n",
       "      <td>드라마</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>베일리 어게인</td>\n",
       "      <td>귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다...</td>\n",
       "      <td>모험</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>주전장</td>\n",
       "      <td>일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인...</td>\n",
       "      <td>다큐멘터리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>포드 V 페라리</td>\n",
       "      <td>1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한...</td>\n",
       "      <td>액션</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     title                                              story  \\\n",
       "0           0      그린 북  1962년 미국 입담과 주먹만 믿고 살아가던 토니 발레롱가비고 모텐슨는 교양과 우아...   \n",
       "1           1      가버나움  나를 세상에 태어나게 한 부모님을 고소하고 싶어요출생기록조차 없이 살아온 어쩌면 1...   \n",
       "2           2   베일리 어게인  귀여운 소년 이든의 단짝 반려견 베일리는 행복한 생을 마감한다하지만 눈을 떠보니 다...   \n",
       "3           3       주전장  일본의 인종차별 문제를 다룬 영상을 올린 후 우익들의 공격 대상이 된 일본계 미국인...   \n",
       "4           4  포드 V 페라리  1960년대 매출 감소에 빠진 포드는 판매 활로를 찾기 위해스포츠카 레이스를 장악한...   \n",
       "\n",
       "   genre  \n",
       "0    드라마  \n",
       "1    드라마  \n",
       "2     모험  \n",
       "3  다큐멘터리  \n",
       "4     액션  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인\n",
    "\n",
    "movie_df['story'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 22078)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(movie_df['story'])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "그린 북        0\n",
      "가버나움        1\n",
      "베일리 어게인     2\n",
      "주전장         3\n",
      "포드 V 페라리    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# index 값 부여한 indices 시리즈 만들기\n",
    "\n",
    "indices = pd.Series(movie_df.index, index = movie_df['title']).drop_duplicates()\n",
    "\n",
    "print(indices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "idx = indices['그린 북']\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사 영화 출력 함수\n",
    "\n",
    "def get_recommendations(title, cosine_Sim = cosine_sim):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return movie_df['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381            국제시장\n",
       "1              가버나움\n",
       "20               헬프\n",
       "355          인사이드 잡\n",
       "148       피아니스트의 전설\n",
       "69     러브 유어셀프 인 서울\n",
       "140       온리 더 브레이브\n",
       "231             칼리토\n",
       "158          트와이스랜드\n",
       "45            클레멘타인\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('그린 북')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296             웨딩 드레스\n",
       "1                 가버나움\n",
       "218              나비 효과\n",
       "166               해바라기\n",
       "375             싱 스트리트\n",
       "116    빌리 엘리어트 뮤지컬 라이브\n",
       "0                 그린 북\n",
       "160      해리 포터와 마법사의 돌\n",
       "354                크로싱\n",
       "50          다시 태어나도 우리\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations('국제시장')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## 잠재 의미 분석 (Latent Semantic Analysis): LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 1000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1000, max_df = 0.5, smooth_idf = True)\n",
    "X = vectorizer.fit_transform(movie_reviews)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_model = TruncatedSVD(n_components = 5, algorithm = 'randomized', \n",
    "                         n_iter = 100, random_state = 122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n = 5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d: \" % (idx + 1),\n",
    "             [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n-1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  [('된다', 0.21939), ('위해', 0.20719), ('있는', 0.16388), ('함께', 0.1546), ('없는', 0.14288)]\n",
      "Topic 2:  [('반지원정대는', 0.28616), ('악의', 0.27503), ('전쟁을', 0.25209), ('위해', 0.22172), ('최후의', 0.16811)]\n",
      "Topic 3:  [('있을까', 0.20423), ('이야기가', 0.15772), ('가장', 0.15716), ('시작된다', 0.14525), ('로봇', 0.13999)]\n",
      "Topic 4:  [('위해', 0.27143), ('되는데', 0.23134), ('세계', 0.20337), ('미국', 0.18014), ('꿈을', 0.13367)]\n",
      "Topic 5:  [('로봇', 0.3401), ('있을까', 0.25133), ('위기에', 0.15261), ('과연', 0.1443), ('된다', 0.12299)]\n"
     ]
    }
   ],
   "source": [
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잠재 디리클레 할당 (Latent Dirichlet Allocation): LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle = True, random_state =1,\n",
    "                             remove = ('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() \n",
    "                                                                      if len(w) > 3]))\n",
    "news_df['clean_doc'] = news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 처리\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words( 'english' )\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 수행과정\n",
    "\n",
    "- 1 사용자는 알고리즘에게 topic 개수 k를 알려준다.\n",
    "- 2 모든 문서를 k개 중 하나의 topic에 할당한다.\n",
    "- 3 이제 모든 문서의 모든 단어에 대해서 다음 사항을 반복해서 진행한다.( iteration )\n",
    "    - 3.1 어떤 문서의 각 단어 w는 자신은 잘못된 topic에 할당되어져 있지만, 다른 단어들은 전부 올바른 topic에 할당되어져 있는 상태라고 가정한다. 이에 따라 단어 w는 다음 기준에 따라 topic이 재할당된다.\n",
    "\n",
    "        - p( topic t | document d ) : 문서 d의 단어들 중 topic t에 해당하는 단어들의 비율\n",
    "        - p( word w | topic t ) : 단어 w를 갖고 있는 모든 문서들 중 topic t가 할당된 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 2), (86, 1), (87, 1), (88, 1), (89, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# gensim을 이용한 정수 인코딩\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력, 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faith\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64281"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.014*\"president\" + 0.013*\"government\" + 0.008*\"technology\" + 0.007*\"administration\"')\n",
      "(1, '0.021*\"font\" + 0.017*\"fonts\" + 0.009*\"mask\" + 0.008*\"bitmap\"')\n",
      "(2, '0.019*\"game\" + 0.017*\"team\" + 0.017*\"year\" + 0.013*\"games\"')\n",
      "(3, '0.012*\"phillies\" + 0.011*\"messiah\" + 0.010*\"powerbook\" + 0.008*\"rows\"')\n",
      "(4, '0.007*\"used\" + 0.007*\"ground\" + 0.006*\"time\" + 0.006*\"engine\"')\n",
      "(5, '0.047*\"space\" + 0.018*\"nasa\" + 0.010*\"launch\" + 0.009*\"earth\"')\n",
      "(6, '0.018*\"would\" + 0.013*\"people\" + 0.011*\"think\" + 0.010*\"like\"')\n",
      "(7, '0.017*\"mail\" + 0.013*\"address\" + 0.012*\"posting\" + 0.012*\"list\"')\n",
      "(8, '0.016*\"data\" + 0.012*\"image\" + 0.011*\"system\" + 0.009*\"available\"')\n",
      "(9, '0.016*\"gordon\" + 0.015*\"pitt\" + 0.014*\"banks\" + 0.013*\"soon\"')\n",
      "(10, '0.014*\"byte\" + 0.011*\"instruction\" + 0.010*\"crash\" + 0.010*\"part\"')\n",
      "(11, '0.020*\"file\" + 0.013*\"program\" + 0.010*\"output\" + 0.009*\"window\"')\n",
      "(12, '0.009*\"israel\" + 0.009*\"government\" + 0.008*\"people\" + 0.008*\"state\"')\n",
      "(13, '0.018*\"armenian\" + 0.017*\"turkish\" + 0.013*\"health\" + 0.012*\"armenians\"')\n",
      "(14, '0.017*\"period\" + 0.010*\"play\" + 0.009*\"power\" + 0.008*\"chicago\"')\n",
      "(15, '0.016*\"said\" + 0.011*\"went\" + 0.008*\"back\" + 0.007*\"people\"')\n",
      "(16, '0.013*\"jesus\" + 0.008*\"christian\" + 0.008*\"believe\" + 0.008*\"bible\"')\n",
      "(17, '0.008*\"condition\" + 0.008*\"sale\" + 0.008*\"book\" + 0.007*\"asking\"')\n",
      "(18, '0.016*\"cubs\" + 0.009*\"emacs\" + 0.008*\"suck\" + 0.007*\"boots\"')\n",
      "(19, '0.011*\"would\" + 0.011*\"thanks\" + 0.010*\"like\" + 0.010*\"drive\"')\n"
     ]
    }
   ],
   "source": [
    "# 토픽별 단어 분포 보기\n",
    "\n",
    "import gensim\n",
    "\n",
    "NUM_TOPICS = 20  # 토픽 개수\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, \n",
    "                                           id2word=dictionary, passes=15) # passes: 알고리즘 동작 횟수\n",
    "topics = ldamodel.print_topics(num_words=4)  # 4개의 단어만 출력 / 안 주면 10개 출력\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LDA 시각화\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서별 topic 분포 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문서의 topic 비율은  [(6, 0.30564585), (12, 0.42042848), (13, 0.059570596), (15, 0.20122413)]\n",
      "1 번째 문서의 topic 비율은  [(2, 0.09468102), (6, 0.49541897), (12, 0.06764484), (13, 0.05399619), (16, 0.24211265), (17, 0.02769197)]\n",
      "2 번째 문서의 topic 비율은  [(6, 0.54967505), (12, 0.32414076), (15, 0.111986026)]\n",
      "3 번째 문서의 topic 비율은  [(0, 0.29661635), (6, 0.39061454), (7, 0.026280891), (8, 0.066823594), (17, 0.1443455), (19, 0.06434831)]\n",
      "4 번째 문서의 topic 비율은  [(2, 0.37888154), (6, 0.2957579), (9, 0.29386693)]\n"
     ]
    }
   ],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i == 5:\n",
    "        break;\n",
    "    print(i, '번째 문서의 topic 비율은 ', topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree\n",
      "guilt\n",
      "ignore\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[9]); print(dictionary[16]); print(dictionary[18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus, texts):\n",
    "    topic_table = pd.DataFrame()\n",
    "    \n",
    "    for i, topic_list in enumerate(ldamodel[corpus]): # 문서 번호 / 토픽 비중\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list\n",
    "        doc = sorted(doc, key = lambda x: (x[1]), reverse = True)  # 각 문서에 대해 비중이 높은순 정렬\n",
    "        \n",
    "        for j, (topic_num, prop_topic) in enumerate(doc):  # 모든 문서에 대해 수행\n",
    "            if j == 0:\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), \n",
    "                                                           round(prop_topic, 4),\n",
    "                                                           topic_list]),\n",
    "                                                           ignore_index = True) # 가장 비중이 높은 토픽과 그 비중, 전체 토픽 비중\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서 번호</th>\n",
       "      <th>가장 높은 비중 토픽</th>\n",
       "      <th>가장 높은 토픽 비중</th>\n",
       "      <th>각 토픽의 비중</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.4204</td>\n",
       "      <td>[(6, 0.30564588), (12, 0.42043912), (13, 0.059...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4954</td>\n",
       "      <td>[(2, 0.09470914), (6, 0.49536663), (12, 0.0676...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5497</td>\n",
       "      <td>[(6, 0.549668), (12, 0.32414025), (15, 0.11199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3906</td>\n",
       "      <td>[(0, 0.29661125), (6, 0.39058194), (7, 0.02628...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>[(2, 0.37878168), (6, 0.29584357), (9, 0.29388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3780</td>\n",
       "      <td>[(5, 0.14903022), (6, 0.37804374), (16, 0.3126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.4022</td>\n",
       "      <td>[(0, 0.036834225), (1, 0.026452355), (4, 0.173...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6520</td>\n",
       "      <td>[(6, 0.6519972), (12, 0.15365508), (13, 0.1803...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5424</td>\n",
       "      <td>[(6, 0.54240465), (14, 0.3165403), (19, 0.1159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.4686</td>\n",
       "      <td>[(4, 0.2061655), (6, 0.29983205), (18, 0.01432...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   문서 번호  가장 높은 비중 토픽  가장 높은 토픽 비중  \\\n",
       "0      0         12.0       0.4204   \n",
       "1      1          6.0       0.4954   \n",
       "2      2          6.0       0.5497   \n",
       "3      3          6.0       0.3906   \n",
       "4      4          2.0       0.3788   \n",
       "5      5          6.0       0.3780   \n",
       "6      6         19.0       0.4022   \n",
       "7      7          6.0       0.6520   \n",
       "8      8          6.0       0.5424   \n",
       "9      9         19.0       0.4686   \n",
       "\n",
       "                                            각 토픽의 비중  \n",
       "0  [(6, 0.30564588), (12, 0.42043912), (13, 0.059...  \n",
       "1  [(2, 0.09470914), (6, 0.49536663), (12, 0.0676...  \n",
       "2  [(6, 0.549668), (12, 0.32414025), (15, 0.11199...  \n",
       "3  [(0, 0.29661125), (6, 0.39058194), (7, 0.02628...  \n",
       "4  [(2, 0.37878168), (6, 0.29584357), (9, 0.29388...  \n",
       "5  [(5, 0.14903022), (6, 0.37804374), (16, 0.3126...  \n",
       "6  [(0, 0.036834225), (1, 0.026452355), (4, 0.173...  \n",
       "7  [(6, 0.6519972), (12, 0.15365508), (13, 0.1803...  \n",
       "8  [(6, 0.54240465), (14, 0.3165403), (19, 0.1159...  \n",
       "9  [(4, 0.2061655), (6, 0.29983205), (18, 0.01432...  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_table = make_topictable_per_doc(ldamodel, corpus, tokenized_doc)\n",
    "topic_table = topic_table.reset_index()\n",
    "topic_table.columns = ['문서 번호', '가장 높은 비중 토픽', '가장 높은 토픽 비중', '각 토픽의 비중']\n",
    "topic_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn을 이용한 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 15년 동안 발행되었던 뉴스 기사 제목 데이터\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\",\n",
    "                         filename = \"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082168\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[['headline_text']]\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, against, community, broadcastin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, be, aware, of, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, g, calls, for, infrastructure, protection,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, in, aust, strike, for, pay, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, to, affect, australian, trav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  [aba, decides, against, community, broadcastin...\n",
       "1  [act, fire, witnesses, must, be, aware, of, de...\n",
       "2  [a, g, calls, for, infrastructure, protection,...\n",
       "3  [air, nz, staff, in, aust, strike, for, pay, r...\n",
       "4  [air, nz, strike, to, affect, australian, trav..."
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Preprocessing\n",
    "# 토큰화\n",
    "\n",
    "import nltk \n",
    "text['headline_text'] = text.apply(lambda row: nltk.word_tokenize(row['headline_text']), \n",
    "                                   axis = 1)\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decides, community, broadcasting, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witnesses, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, calls, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0   [aba, decides, community, broadcasting, licence]\n",
       "1    [act, fire, witnesses, must, aware, defamation]\n",
       "2     [g, calls, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word \n",
    "                                                               in x \n",
    "                                                               if word not in (stop)])\n",
    "\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decide, community, broadcast, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witness, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, call, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0       [aba, decide, community, broadcast, licence]\n",
       "1      [act, fire, witness, must, aware, defamation]\n",
       "2      [g, call, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표제어 추출(3인칭 단수 -> 1인칭, 과거 -> 현재형)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') \n",
    "                                                               for word in x])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[aba, decide, community, broadcast, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[act, fire, witness, must, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[g, call, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0       [aba, decide, community, broadcast, licence]\n",
       "1      [act, fire, witness, must, aware, defamation]\n",
       "2      [g, call, infrastructure, protection, summit]\n",
       "3          [air, nz, staff, aust, strike, pay, rise]\n",
       "4  [air, nz, strike, affect, australian, travellers]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이 3 이하인 단어 제거\n",
    "\n",
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if\n",
    "                                                       len(word) > 3])\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "## TF-IDF 행렬\n",
    "# 역 토큰화\n",
    "\n",
    "detokenized_doc = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       decide community broadcast licence\n",
       "1       fire witness must aware defamation\n",
       "2    call infrastructure protection summit\n",
       "3                   staff aust strike rise\n",
       "4      strike affect australian travellers\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['headline_text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082168, 1000)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1000개의 단어로 TF-IDF 행렬 만들기\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 1000)\n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic Modeling\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components = 10, \n",
    "                                     learning_method = 'online',\n",
    "                                     random_state = 777,\n",
    "                                     max_iter = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top = lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00001533e-01 1.00001269e-01 1.00004179e-01 ... 1.00006124e-01\n",
      "  1.00003111e-01 1.00003064e-01]\n",
      " [1.00001199e-01 1.13513398e+03 3.50170830e+03 ... 1.00009349e-01\n",
      "  1.00001896e-01 1.00002937e-01]\n",
      " [1.00001811e-01 1.00001151e-01 1.00003566e-01 ... 1.00002693e-01\n",
      "  1.00002061e-01 7.53381835e+02]\n",
      " ...\n",
      " [1.00001065e-01 1.00001689e-01 1.00003278e-01 ... 1.00006721e-01\n",
      "  1.00004902e-01 1.00004759e-01]\n",
      " [1.00002401e-01 1.00000732e-01 1.00002989e-01 ... 1.00003517e-01\n",
      "  1.00001428e-01 1.00005266e-01]\n",
      " [1.00003427e-01 1.00002313e-01 1.00007340e-01 ... 1.00003732e-01\n",
      "  1.00001207e-01 1.00005153e-01]]\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('government', 8725.19), ('sydney', 8393.29), ('queensland', 7720.12), ('change', 5874.27), ('home', 5674.38)]\n",
      "Topic 2: [('australia', 13691.08), ('australian', 11088.95), ('melbourne', 7528.43), ('world', 6707.7), ('south', 6677.03)]\n",
      "Topic 3: [('death', 5935.06), ('interview', 5924.98), ('kill', 5851.6), ('jail', 4632.85), ('life', 4275.27)]\n",
      "Topic 4: [('house', 6113.49), ('2016', 5488.19), ('state', 4923.41), ('brisbane', 4857.21), ('tasmania', 4610.97)]\n",
      "Topic 5: [('court', 7542.74), ('attack', 6959.64), ('open', 5663.0), ('face', 5193.63), ('warn', 5115.01)]\n",
      "Topic 6: [('market', 5545.86), ('rural', 5502.89), ('plan', 4828.71), ('indigenous', 4223.4), ('power', 3968.26)]\n",
      "Topic 7: [('charge', 8428.8), ('election', 7561.63), ('adelaide', 6758.36), ('make', 5658.99), ('test', 5062.69)]\n",
      "Topic 8: [('police', 12092.44), ('crash', 5281.14), ('drug', 4290.87), ('beat', 3257.58), ('rise', 2934.92)]\n",
      "Topic 9: [('fund', 4693.03), ('labor', 4047.69), ('national', 4038.68), ('council', 4006.62), ('claim', 3604.75)]\n",
      "Topic 10: [('trump', 11966.41), ('perth', 6456.53), ('report', 5611.33), ('school', 5465.06), ('woman', 5456.76)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # 단어 집합, 1000개의 단어 저장\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i \n",
    "                                     in topic.argsort()[:-n-1:-1]])\n",
    "get_topics(lda_model.components_,terms)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSA: DTM을 차원 축소하여 근접 단어들을 토픽으로 묶는다.\n",
    "- LDA: 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
