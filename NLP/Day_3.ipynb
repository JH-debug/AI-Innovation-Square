{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Topic >\n",
    "\n",
    "1. review\n",
    "    \n",
    "    1.1 1일차 내용에 대한 review (naver 영화 한줄평 이용)\n",
    "\n",
    "\n",
    "2. topic\n",
    "    \n",
    "    2.1 정수 인코딩 (Integer encoding)\n",
    "    \n",
    "    2.2 원 핫 인코딩\n",
    "    \n",
    "    2.3 Byte Pair Encoding (BPE)\n",
    "    \n",
    "    2.4 데이터 분리 (Splitting  data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 참고) 강사님 메일: wolee777@aicore.co.kr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "# 1. Naver 영화 한줄평 crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://movie.naver.com/movie/bi/mi/basic.nhn?code=187321#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url)\n",
    "html = BeautifulSoup(resp.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_result = html.find('div', {'class' : 'score_result'})\n",
    "lis = score_result.findAll('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이 영화는 미쳤다. 넷플릭스가 일상화된 시대에 극장이 존재해야하는 이유를 증명해준다.', '충무로: 이거 어케하는거냐?', '아카데미에서 촬영상, 음향효과상, 시각효과상을 받은 이유가 고스란히 녹아있는 영화. IMAX로 관람하는걸 추천한다.', '촬영감독의 영혼까지 갈아넣은 마스터피스', '오스카 작품상 탔어도 할말 없었을것 같다.']\n"
     ]
    }
   ],
   "source": [
    "# li 태그가 여러 개 이므로 반복문으로 리스트에 담음\n",
    "# strip(): 문자열 앞뒤 공백을 제거하고 내용만 가져오는 함수\n",
    "\n",
    "comments = []\n",
    "for li in lis:\n",
    "    review_text = li.find('p').getText()\n",
    "    comments.append(review_text.strip())\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화는 미쳤다. 넷플릭스가 일상화된 시대에 극장이 존재해야하는 이유를 증명해준다.\n",
      "\n",
      "충무로: 이거 어케하는거냐?\n",
      "\n",
      "아카데미에서 촬영상, 음향효과상, 시각효과상을 받은 이유가 고스란히 녹아있는 영화. IMAX로 관람하는걸 추천한다.\n",
      "\n",
      "촬영감독의 영혼까지 갈아넣은 마스터피스\n",
      "\n",
      "오스카 작품상 탔어도 할말 없었을것 같다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 줄바꿈 기호를 넣어 보기 좋게 처리\n",
    "\n",
    "for comment in comments:\n",
    "    print(comment + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Crawling 결과를 파일에 저장 / 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위로 저장\n",
    "# text 파일에 저장 시 한 줄씩 저장하는 것이 좋음\n",
    "\n",
    "with open ('comment_1917.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for comment in comments:\n",
    "        f.write( comment + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이 영화는 미쳤다. 넷플릭스가 일상화된 시대에 극장이 존재해야하는 이유를 증명해준다.', '충무로: 이거 어케하는거냐?', '아카데미에서 촬영상, 음향효과상, 시각효과상을 받은 이유가 고스란히 녹아있는 영화. IMAX로 관람하는걸 추천한다.', '촬영감독의 영혼까지 갈아넣은 마스터피스', '오스카 작품상 탔어도 할말 없었을것 같다.']\n"
     ]
    }
   ],
   "source": [
    "# readlines(): 실제 읽어들이는 내용 - 전체 내용을 list로 생성\n",
    "# rstrip: 줄바꿈 기호 제거 - 나중에 처리할 때 문제될 수 있음\n",
    "\n",
    "movie_comments = []\n",
    "\n",
    "with open('comment_1917.txt', 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        movie_comments.append(line.rstrip('\\n'))\n",
    "\n",
    "print(movie_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 결과 리스트 내용을 하나의 문자열로 변경\n",
    "리스트는 벡터 파일이기 때문에 하나씩 인덱스로 접근해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화는 미쳤다. 넷플릭스가 일상화된 시대에 극장이 존재해야하는 이유를 증명해준다.충무로: 이거 어케하는거냐?아카데미에서 촬영상, 음향효과상, 시각효과상을 받은 이유가 고스란히 녹아있는 영화. IMAX로 관람하는걸 추천한다.촬영감독의 영혼까지 갈아넣은 마스터피스오스카 작품상 탔어도 할말 없었을것 같다.\n"
     ]
    }
   ],
   "source": [
    "movie_comment = ''\n",
    "\n",
    "for comment in movie_comments:\n",
    "    movie_comment += comment\n",
    "\n",
    "print(movie_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 형태소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '영화', '는', '미쳤다', '.', '넷플릭스', '가', '일상', '화', '된', '시대', '에', '극장', '이', '존재', '해야하는', '이유', '를', '증명', '해준다', '.', '충무로', ':', '이', '거', '어케', '하는거냐', '?', '아카데미', '에서', '촬영상', ',', '음향효과', '상', ',', '시', '각', '효과', '상', '을', '받은', '이유', '가', '고스', '란', '히', '녹아있는', '영화', '.', 'IMAX', '로', '관람', '하', '는걸', '추천', '한다', '.', '촬영감독', '의', '영혼', '까지', '갈아', '넣은', '마스터피스', '오스카', '작품', '상', '탔어도', '할말', '없었을것', '같다', '.']\n"
     ]
    }
   ],
   "source": [
    "morphemes = []\n",
    "\n",
    "for morphs in okt.morphs(movie_comment):\n",
    "    morphemes.append(morphs)\n",
    "\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 형태소 추출 및 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '영화', '는', '미치다', '.', '넷플릭스', '가', '일상', '화', '되다', '시대', '에', '극장', '이', '존재', '하다', '이유', '를', '증명', '해주다', '.', '충무로', ':', '이', '거', '어케', '하다', '?', '아카데미', '에서', '촬영상', ',', '음향효과', '상', ',', '시', '각', '효과', '상', '을', '받다', '이유', '가', '고스', '란', '히', '녹다', '영화', '.', 'IMAX', '로', '관람', '하', '는걸', '추천', '하다', '.', '촬영감독', '의', '영혼', '까지', '갈아', '넣다', '마스터피스', '오스카', '작품', '상', '타다', '하다', '없다', '같다', '.']\n"
     ]
    }
   ],
   "source": [
    "morphemes = []\n",
    "\n",
    "for morphs in okt.morphs(movie_comment, stem = True):\n",
    "    morphemes.append(morphs)\n",
    "\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 형태소 추출 및 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '영화', '는', '미쳤다', '넷플릭스', '가', '일상', '화', '된', '시대', '에', '극장', '이', '존재', '해야하는', '이유', '를', '증명', '해준다', '충무로', '이', '거', '어떻게', '하는거냐', '아카데미', '에서', '촬영상', '음향효과', '상', '시', '각', '효과', '상', '을', '받은', '이유', '가', '고스', '란', '히', '녹아있는', '영화', 'IMAX', '로', '관람', '하', '는걸', '추천', '한다', '촬영감독', '의', '영혼', '까지', '갈아', '넣은', '마스터피스', '오스카', '작품', '상', '탔어도', '할말', '없었을것', '같다']\n"
     ]
    }
   ],
   "source": [
    "morphemes = []\n",
    "\n",
    "for morphs in okt.morphs(movie_comment, norm = True):\n",
    "    morphemes.append(morphs)\n",
    "\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning: 형태소 분석 이전, preprocessing 등 언제나 할 수 있음\n",
    "# 꼭 cleaning 이후 형태소 분석을 할 필요는 없음\n",
    "# noise data 제거는 항상 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 명사 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '영화', '넷플릭스', '일상', '시대', '극장', '존재', '이유', '증명', '충무로', '거', '어케', '아카데미', '촬영상', '음향효과', '효과', '이유', '고스', '란', '영화', '로', '관람', '는걸', '추천', '촬영감독', '영혼', '마스터피스', '오스카', '작품']\n"
     ]
    }
   ],
   "source": [
    "morphemes = []\n",
    "\n",
    "for morphs in okt.nouns(movie_comment):\n",
    "    morphemes.append(morphs)\n",
    "\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 어절 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이 영화', '넷플릭스', '일상화', '일상화된 시대', '극장', '존재', '존재해야하는 이유', '증명', '충무로', '충무로 이거', '충무로 이거 어케', '아카데미', '촬영상', '촬영상 음향효과상', '촬영상 음향효과상 시각효과상', '이유', '고스란', '영화', '영화 IMAX로', '영화 IMAX로 관람하는걸', '영화 IMAX로 관람하는걸 추천', '촬영감독', '촬영감독의 영혼', '마스터피스오스카', '마스터피스오스카 작품상', '시대', '이거', '어케', '음향효과상', '시각효과상', '고스', 'IMAX', '관람하', '는걸', '추천', '영혼', '마스터피스', '오스카', '작품상']\n"
     ]
    }
   ],
   "source": [
    "morphemes = []\n",
    "\n",
    "for morphs in okt.phrases(movie_comment):\n",
    "    morphemes.append(morphs)\n",
    "\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이/Noun', '영화/Noun', '는/Josa', '미쳤다/Adjective', '넷플릭스/Noun', '가/Josa', '일상/Noun', '화/Suffix', '된/Verb', '시대/Noun', '에/Josa', '극장/Noun', '이/Josa', '존재/Noun', '해야하는/Verb', '이유/Noun', '를/Josa', '증명/Noun', '해준다/Verb', '충무로/Noun', '이/Determiner', '거/Noun', '어케/Noun', '하는거냐/Verb', '아카데미/Noun', '에서/Josa', '촬영상/Noun', '음향효과/Noun', '상/Suffix', '시/Modifier', '각/Modifier', '효과/Noun', '상/Suffix', '을/Josa', '받은/Verb', '이유/Noun', '가/Josa', '고스/Noun', '란/Noun', '히/Adverb', '녹아있는/Verb', '영화/Noun', 'IMAX/Alpha', '로/Noun', '관람/Noun', '하/Suffix', '는걸/Noun', '추천/Noun', '한다/Verb', '촬영감독/Noun', '의/Josa', '영혼/Noun', '까지/Josa', '갈아/Adverb', '넣은/Verb', '마스터피스/Noun', '오스카/Noun', '작품/Noun', '상/Suffix', '탔어도/Verb', '할말/Verb', '없었을것/Adjective', '같다/Adjective']\n"
     ]
    }
   ],
   "source": [
    "morphemes = []\n",
    "\n",
    "for morphs in okt.pos(movie_comment, join =True):  # join 옵션을 안 주면 태그(/Noun)가 같이 안 나옴\n",
    "    morphemes.append(morphs)\n",
    "\n",
    "print(morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 불용어 처리 방법 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 불용어 파일로 직접 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ranks.nl/stopwords/korean\n",
    "\n",
    "- 위 사이트에서 불용어 복사해, visual studio, 메모장 등에 붙여넣고 파일 생성\n",
    "- 마지막 줄 비워놓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 문은 자동으로 close 수행 (파일 읽고, append할 때 유용)\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "with open('stopwords.txt', 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        stop_words.append(line.rstrip('\\n'))\n",
    "        \n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영화', '는', '미쳤다', '.', '넷플릭스', '일상', '화', '된', '시대', '극장', '존재', '해야하는', '이유', '증명', '해준다', '.', '충무로', ':', '거', '어케', '하는거냐', '?', '아카데미', '촬영상', ',', '음향효과', '상', ',', '시', '효과', '상', '받은', '이유', '고스', '란', '히', '녹아있는', '영화', '.', 'IMAX', '관람', '는걸', '추천', '한다', '.', '촬영감독', '영혼', '갈아', '넣은', '마스터피스', '오스카', '작품', '상', '탔어도', '할말', '없었을것', '.']\n"
     ]
    }
   ],
   "source": [
    "# 아래 데이터에는 .과 ?가 포함되는 문제점 (noise data)\n",
    "\n",
    "results = []\n",
    "\n",
    "for word in okt.morphs(movie_comment):\n",
    "    if word not in stop_words:\n",
    "        results.append(word)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 처리 방법 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 정규식 이용 (cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 영화는 미쳤다 넷플릭스가 일상화된 시대에 극장이 존재해야하는 이유를 증명해준다충무로 이거 어케하는거냐아카데미에서 촬영상 음향효과상 시각효과상을 받은 이유가 고스란히 녹아있는 영화 IMAX로 관람하는걸 추천한다촬영감독의 영혼까지 갈아넣은 마스터피스오스카 작품상 탔어도 할말 없었을것 같다\n"
     ]
    }
   ],
   "source": [
    "r = '[-=+,#/\\?:^$.@*\\\"~&%.!\\'|\\(\\)\\[\\]\\<\\>`\\']'\n",
    "\n",
    "movie_comment = re.sub(r, '', movie_comment)\n",
    "print(movie_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 불용어 처리 방법 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - NLTK.word_tokenize() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '영화는', '미쳤다', '넷플릭스가', '일상화된', '시대에', '극장이', '존재해야하는', '이유를', '증명해준다충무로', '이거', '어케하는거냐아카데미에서', '촬영상', '음향효과상', '시각효과상을', '받은', '이유가', '고스란히', '녹아있는', '영화', 'IMAX로', '관람하는걸', '추천한다촬영감독의', '영혼까지', '갈아넣은', '마스터피스오스카', '작품상', '탔어도', '할말', '없었을것', '같다']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(movie_comment)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영화는', '미쳤다', '넷플릭스가', '일상화된', '시대에', '극장이', '존재해야하는', '이유를', '증명해준다충무로', '이거', '어케하는거냐아카데미에서', '촬영상', '음향효과상', '시각효과상을', '받은', '이유가', '고스란히', '녹아있는', '영화', 'IMAX로', '관람하는걸', '추천한다촬영감독의', '영혼까지', '갈아넣은', '마스터피스오스카', '작품상', '탔어도', '할말', '없었을것']\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        results.append(word)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) konlpy 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참조: http://konlpy.org/ko/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "from konlpy.corpus import kobill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국헌법\\n\\n유구한 역사와 전통에 빛나는 우리 대한국'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kolaw.open( 'constitution.txt' ).read()[ :30 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지방공무원법 일부개정법률안\\n\\n(정의화의원 대표발의 )\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobill.open( '1809890.txt' ).read()[ :30 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 한나눔 (Hannanum) 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '자연어', '처리', '공부']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.nouns('나는 자연어 처리를 공부한다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나', 'N'),\n",
       " ('는', 'J'),\n",
       " ('자연어', 'N'),\n",
       " ('처리', 'N'),\n",
       " ('를', 'J'),\n",
       " ('공부', 'N'),\n",
       " ('하', 'X'),\n",
       " ('ㄴ다', 'E'),\n",
       " ('.', 'S')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.pos('나는 자연어 처리를 공부한다.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic: Encoding\n",
    "\n",
    "### 컴퓨터의 인식을 수월히 하기 위해서는 인코딩 필요 (텍스트 -> 숫자)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 정수 인코딩 (Integer encoding)\n",
    "\n",
    "- 텍스트를 정수로 변환하는 방법\n",
    "- 텍스트의 빈도수를 확인해서 빈도수에 따른 index 부여 (decending)\n",
    "- 단어를 빈도수 순으로 정렬한 단어 집합 (vocabulary)을 만들고, 빈도수가 높은 순서부터 차례로 낮은 숫자의 정수를 부여하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) dictionary 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 텍스트 전처리: 토큰화, 불용어 처리, cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 정제와 단어 토큰화\n",
    "\n",
    "vocab = {}  # dictionary 자료형: 단어-빈도수 저장\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i)  # 단어 토큰화\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        word = word.lower()  # 소문자 변환 함수\n",
    "        if word not in stop_words:  # 불용어 제거\n",
    "            if len(word) > 2:   # 단어 길이가 2 이하인 경우, 단어 제거\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0   # 단어 없으면 0\n",
    "                vocab[word] += 1     # 단어 있으면 +1\n",
    "    sentences.append(result)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 빈도수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 해당 단어의 빈도수 확인 가능\n",
    "\n",
    "print(vocab['good'])\n",
    "print(vocab['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 빈도수가 높은 순서대로 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# sorted() 함수\n",
    "# dictionary는 key를 대상으로 할 것인지, value를 대상으로 할 것인지 정해야 함\n",
    "# items(): 항목 하나씩 꺼냄\n",
    "# sorted() 함수의 key라는 인수에 정렬식을 넣음\n",
    "# lambda 인수(x):return 값(x[1]) -> 1번째 데이터를 꺼냄\n",
    "\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x: x [1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab_sorted:   # dictionary는 key, value로 이루어짐\n",
    "    if frequency > 1:  # 빈도수가 일정 이하인 단어는 제외\n",
    "        i += 1\n",
    "        word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 상위 빈도 단어만 저장 (5위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# list comprehension 사용: 특별한 제어문 없이 데이터 구조 손쉽게 표현\n",
    "# w: key(단어) / c: value(빈도수)\n",
    "\n",
    "vocab_size = 5\n",
    "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size + 1]\n",
    "\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]  # 해당 단어에 대한 인덱스 정보 삭제\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Out-Of-Vocabulary (OOV, 단어 집합에 없는 단어)\n",
    "train, test set에는 있으나 단어 집합에 존재하지 않는 단어들(저빈도 단어들)의 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'OOV' 추가\n",
    "\n",
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Mapping: word_to_index(단어: 빈도수)로 sentences(전처리 후 데이터)의 모든 단어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:  # sentences(토믄화된 모든 단어) 중 word_to_index의 key값 검색\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:   # word_to_index의 key에 포함되지 않는 단어들\n",
    "            temp.append(word_to_index['OOV'])   # OOV에 담음\n",
    "    encoded.append(temp)\n",
    "\n",
    "print(encoded)   # 결과 값은 각 문장들을 치환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Counter 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 단어 집합(vocabulary) 만들기: sentences에서 문장의 경계인 '[ ]'를 제거하고 단어 전부를 담은 하나의 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# sum(iterable): 입력받은 리스트나 튜플의 모든 요소의 합을 돌려주는 함수\n",
    "\n",
    "words = sum(sentences, [])  # '[]'는 제외하고 새로운 리스트를 만듦\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Counter()을 이용하여 중복을 제거하고 빈도수 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - most_common(): 상위 빈도 단어만 저장 (5위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) ; vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 상위 빈도 단어일수록 낮은 정수 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "\n",
    "for (word, frequency) in vocab:   # dictionary는 key, value로 이루어짐\n",
    "    i += 1\n",
    "    word_to_index[word] = i\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 이후, OOV 처리와 mapping까지 하면 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) NLTK의 FreqDist 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 빈도수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, ...})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.hstack(): 문장 구분 제거\n",
    "# FreqDist 형식으로 객체를 담음\n",
    "\n",
    "vocab = FreqDist(np.hstack(sentences)) ; vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - most_common(): 상위 빈도 단어만 저장 (5위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) ; vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 상위 빈도 단어일수록 낮은 정수 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# list comprehension 형식\n",
    "# key: value\n",
    "# enumerate: 데이터를 차례대로 꺼냄\n",
    "\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 이후, OOV 처리와 mapping까지 하면 완료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고) enumerate 함수\n",
    "\n",
    "- Iterator: 반복자, 반복 기능을 포함한 객체\n",
    "           Sequence 자료형: list, tuple, dictionary, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n",
      "--------------------\n",
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "# enunemrate 사용\n",
    "\n",
    "test = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "for index, value in enumerate(test):\n",
    "    print(\"value : {}, index: {}\".format(value, index))\n",
    "\n",
    "print('--------------------')\n",
    "    \n",
    "# enumerate 사용 X\n",
    "\n",
    "index = 0\n",
    "for value in test:\n",
    "    print(\"value : {}, index: {}\".format(value, index))\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# + Keras가 제공하는 Text preprocessing\n",
    "### - tokenize / 빈도수에 따른 index 부여 / 정수 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 생성\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# fir_on_texts(): 빈도수 기준으로 단어 집합을 생성하는 함수(method)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "# 각 단어에 부여된 인덱스 확인\n",
    "\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 각 단어의 개수 확인\n",
    "\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 빈도수에 따른 상위 5개 추출 / 정수 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])\n",
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "# 삭제할 대상\n",
    "words_frequency = [w for w, c in tokenizer.word_index.items() if c >= vocab_size + 1]\n",
    "\n",
    "# 삭제\n",
    "for w in words_frequency:\n",
    "    del tokenizer.word_index[w]\n",
    "    del tokenizer.word_counts[w]\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 더 간단한 방법) 빈도수에 따른 상위 5개 추출 / 정수 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size+1)  # (0~4) 상위 5개만 사용\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. One-hot Encoding\n",
    "- 단어 집합의 크기를 벡터(1차원 배열)의 차원으로 표현\n",
    "- 0과 1로 표현\n",
    "- \"정수 인코딩\"을 한 후, \"원핫 인코딩\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 집합( 5 : 빈도수가 높은 상위 단어 수): ['barber', 'person', 'huge', 'kept', 'secret']\n",
    "\n",
    "단어 집합의 단어에 대한 인덱스 부여: [ 1   /    2    /    3    /    4    /    5  ]\n",
    "   \n",
    "정수 인코딩:                [10000  /  01000  /   00100  /  00010  /  00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1) tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "token = okt.morphs('나는 자연어 처리를 배운다')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 2) 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "# 여기서는 빈도수 의미 없음\n",
    "\n",
    "word2index = {}\n",
    "\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 3) One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0]*(len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index]=1\n",
    "\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\", word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# + Keras가 제공하는 One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 1) 정수 인코딩 (단어 집합 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "sub_text = '점심 먹으러 갈래 메뉴는 햄버거 최고야'\n",
    "\n",
    "encoded = t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 2) One-hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞줄의 0은 keras에서 0을 먼저 부여하는 특징 때문\n",
    "# one-hot vector size: 7개이므로 8(7+1)열\n",
    "# '점심 먹으러 갈래 메뉴는 햄버거 최고야' -> 6개의 token: 6행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 단어 집합에 인덱스가 부여된 단어 수만큼 one-hot vector size가 결정 (메모리 낭비)\n",
    "\n",
    "2. 유사 단어에 대한 처리가 불가능\n",
    "\n",
    "3. one-hot encoding 한계점을 극복하기 위해 사용하는 방법: \n",
    " \n",
    "    - 카운트 기반 벡터 방식\n",
    "    - 예측 기반 벡터 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BPE(Byte Pair Encoding)\n",
    "\n",
    "- OOV (Out-Of-Vocabulary) / UNK(Unknown Token) 문제 해결\n",
    "- 압축 알고리즘: 중복되는 부분을 줄임\n",
    "- 중복되는 단어를 모아 압축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 과제: \n",
    "1. 네이버 영화 한줄평 크롤링 결과를 이용해서 2, 3일차 내용 적용\n",
    "2. 네이버 영화에서 영화별로 줄거리를 크롤링하고, 각각을 파일로 저장, 그 결과를 이용해서 2, 3일차 내용 적용\n",
    "\n",
    "(tokenize, normalization, stop_words, integer encoding, one-hot-encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
