{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Topic >\n",
    "\n",
    "1. review\n",
    "\n",
    "\n",
    "2. topic\n",
    "\n",
    "    2.1 순환 신경망 (Recurrent Neural Network, RNN)\n",
    "\n",
    "    2.2 장단기 메모리 (Long Short-Term Memory, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense( 1, input_dim = 3, activation = 'relu' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense( 8, input_dim = 4, activation = 'relu' ) )\n",
    "model.add(Dense( 1, activation = 'sigmoid' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 은닉층: ht = tanh(WxXt + Whht-1 + b)를 파이썬으로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_state_t = 0 -> 초기 은닉 상태: 0으로 초기화\n",
    "- for input_t in input_length -> 각 시점마다 입력\n",
    "    - output_t = tanh(input_t, hidden_state_t) -> 각 시점에 대한 은닉상태 연산\n",
    "    - hidden_state_t = output_t -> 현재 시점의 은닉상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수, NLP 관점에서는 문장의 길이\n",
    "input_dim = 4 # 입력 차원, NLP 관점에서는 단어 벡터의 차원\n",
    "hidden_size = 8 # 은닉 상태의 크기, 메모리 셀의 용량\n",
    "\n",
    "inputs = np.random.random((timesteps, input_dim)) # 입력에 해당하는 2D 텐서\n",
    "hidden_state_t = np.zeros((hidden_size, )) # 초기 은닉 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state_t)  # 초기 은닉 상태로 모든 차원의 값이 0을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wx = np.random.random((hidden_size, input_dim))  # (8,4) 2D 텐서, 입력 가중치\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8,8) 2D 텐서, 은닉 가중치\n",
    "b = np.random.random((hidden_size,))  # (8,) 크기의 1D 텐서 생성 (bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(Wx)), print(np.shape(Wh)), print(np.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.99998896 0.99998321 0.99998343 0.99971733 0.99875751 0.99996701\n",
      "  0.9999265  0.99999639]\n",
      " [0.99995918 0.99996468 0.99995819 0.99958761 0.99836939 0.99996227\n",
      "  0.99972046 0.99999166]\n",
      " [0.99998982 0.99998495 0.99998458 0.99973    0.99881771 0.99996462\n",
      "  0.99993581 0.99999651]\n",
      " [0.99999436 0.99998428 0.99999239 0.99987536 0.99844026 0.99998969\n",
      "  0.99993032 0.99999889]\n",
      " [0.99999109 0.99998233 0.99998541 0.9995036  0.99865307 0.99996542\n",
      "  0.9999364  0.99999505]\n",
      " [0.99999654 0.9999879  0.99999384 0.99987866 0.99868408 0.99998858\n",
      "  0.99996154 0.99999911]\n",
      " [0.99998508 0.99997218 0.99998028 0.99950327 0.99828028 0.9999753\n",
      "  0.99986415 0.99999453]\n",
      " [0.99999576 0.9999822  0.99999001 0.99978365 0.99864475 0.99998669\n",
      "  0.99995435 0.99999863]\n",
      " [0.99999689 0.99999019 0.99999413 0.99989585 0.9988573  0.99998625\n",
      "  0.99997049 0.99999918]\n",
      " [0.99998744 0.99997339 0.99997685 0.99972138 0.9985933  0.99997837\n",
      "  0.99989388 0.99999704]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "for input_t in inputs:\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
    "    \n",
    "    total_hidden_states.append(list(output_t))\n",
    "    \n",
    "    print(np.shape(total_hidden_states))\n",
    "    \n",
    "    hidden_state_t = output_t\n",
    "    \n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
    "\n",
    "print(total_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Keras로 RNN 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가장 간단한 RNN 표현\n",
    "\n",
    "- model.add(SimpleRNN(hidden_size))\n",
    "- model.add(SimpleRNN(hidden_size, input_shape = (timesteps, input_dim)))\n",
    "- model.add(SimpleRNN(hidden_size, input_length = M, input_dim = N)) # M, N은 정수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(3, input_shape = (2, 10)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(3, batch_input_shape = (8, 2, 10)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_6 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8, 2, 10), return_sequences=True))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 깊은 순환 신경망 (Deep Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model = Sequential()\n",
    "- model.add(SimpleRNN(hidden_size, return_sequences=True))\n",
    "- model.add(SimpleRNN(hidden_size, return_sequences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 양방향 순환 신경망(Bidirectional Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model = Sequential()\n",
    "- model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
